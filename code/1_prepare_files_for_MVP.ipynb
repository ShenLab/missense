{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate files for training and prediction in MVP models.\n",
    "# raw input is WSGA selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import numpy as np\n",
    "import sys\n",
    "import Bio.SubsMat.MatrixInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_score(a_0, a_1, name_matrix=\"blosum62\"):\n",
    "    \"\"\"\n",
    "    Input: a str a_0, a str a_1, a str name_matrix\n",
    "    Output: The matrix score of a_0 and a_1 in the matrix name_matrix\n",
    "    \"\"\"\n",
    "    # Biopython also included placeholder amino acids\n",
    "    #(B. J, X, Z in its scoring matrix.\n",
    "    matrix = getattr(Bio.SubsMat.MatrixInfo, name_matrix)\n",
    "    # Since PAM250 in Biopython is not symmetric, if (a_0, a_1) does not exist,\n",
    "    # matrix_score() will check if (a_1, a_0) exists.\n",
    "    if (a_0, a_1) in matrix:\n",
    "        return matrix[(a_0, a_1)]\n",
    "    elif (a_1, a_0) in matrix:\n",
    "        return matrix[(a_1, a_0)]\n",
    "    else:\n",
    "        return -1    \n",
    "        \n",
    "FASTA_LOC = '/data/hq2130/large_files/resources/hg19.fasta'\n",
    "def add_gc_content(info):\n",
    "    chrom, pos = info['hg19_chr'], int(info['hg19_pos(1-based)'])\n",
    "    fastafile = pysam.Fastafile(FASTA_LOC)\n",
    "    seq = fastafile.fetch(chrom, pos - 5, pos + 5).upper()\n",
    "    gc_count = 0\n",
    "    for dna in seq:\n",
    "        if dna in {'G', 'C'}:\n",
    "            gc_count += 1\n",
    "    gc_count = gc_count / 10.0\n",
    "    info['gc_content'] = gc_count\n",
    "    return info\n",
    "\n",
    "def add_s_het(info):\n",
    "    gene = info['genename']\n",
    "    info['s_het_log'] = -1\n",
    "    if gene in s_het:\n",
    "        info['s_het_log'] = np.log(s_het[gene] + 1)  # minimal value of s_het = 0.000206342\n",
    "    return info\n",
    "\n",
    "def add_gene_metric(info):\n",
    "    info['pli'] = pli.get(info['genename'], -1)\n",
    "    info['lofz'] = lofz.get(info['genename'], -1)\n",
    "    info['prec'] = prec.get(info['genename'], -1)\n",
    "    info['domino'] = domino.get(info['genename'], -1)\n",
    "    return info\n",
    "\n",
    "def add_target(info, target):\n",
    "    info['target'] = target_value\n",
    "    if target_value == 'NA' and 'cancer_target' in info:\n",
    "        info['target'] = int(info['cancer_target'])  \n",
    "    elif target_value == 'NA' and 'category' in info:\n",
    "        if info['category'] == 'TP':\n",
    "            info['target'] = 1\n",
    "        elif info['category'] == 'TN':\n",
    "            info['target'] = 0 \n",
    "    return info\n",
    "\n",
    "def add_gnomad(info):\n",
    "    var_id = info['var_id']\n",
    "    info['gnomad']= gnomad_af.get(var_id, 0) \n",
    "    info['gnomad_exome']= gnomad_exome_af.get(var_id, 0) \n",
    "    return info\n",
    "\n",
    "def add_secondary(info):\n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    info['secondary_H'] = 0\n",
    "    info['secondary_C'] = 0\n",
    "    info['secondary_E'] = 0\n",
    "    if gene in secondary:\n",
    "        aapos = info['aapos'].split(';')\n",
    "        for pos in aapos:\n",
    "            pos = int(pos)\n",
    "            # AA_seq start from 0(it's a list)\n",
    "            protein_length = len(AA_seq[gene])\n",
    "            if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                if pos in secondary[gene]:\n",
    "                    if secondary[gene][pos] == 'H':\n",
    "                        info['secondary_H'] = 1\n",
    "                    elif secondary[gene][pos] == 'C':\n",
    "                        info['secondary_C'] = 1    \n",
    "                    elif secondary[gene][pos] == 'E':\n",
    "                        info['secondary_E'] = 1\n",
    "    return info\n",
    "\n",
    "\n",
    "def add_BioPlex(info):\n",
    "    ''' some feather (BioPlex 2.0) related to protein? added all pint \n",
    "       https://bioplex.hms.harvard.edu/interactions.php\n"
    "    '''\n",
    "    gene = info['genename']\n",
    "    info['BioPlex'] = BioPlex.get(gene, 0)\n",
    "    return info\n",
    "\n",
    "REVEL = '/data/hq2130/large_files/revel_file/revel_all_chr.txt.gz'  # REVEL loc\n",
    "f_revel= pysam.TabixFile(REVEL)\n",
    "def add_REVEL(info):\n",
    "    info['REVEL'] = -1\n",
    "    chrom, pos = info['hg19_chr'], info['hg19_pos(1-based)']\n",
    "    ref, alt = info['ref'], info['alt']\n",
    "    for row in f_revel.fetch(chrom, int(pos)-1, int(pos)+1):# 0-based inputin .fetch\n",
    "        row = row.split('\\t')\n",
    "        if row[3] == ref and row[4] == alt:\n",
    "            info['REVEL'] = row[5]\n",
    "    return info  \n",
    "\n",
    "MPC = '/data/hq2130/large_files/fordist_constraint_official_mpc_values.txt.gz'  # mpc\n",
    "f_MPC = pysam.TabixFile(MPC)\n",
    "def add_MPC(info):\n",
    "    info['MPC'] = -1\n",
    "    info['mis_badness'] = -1\n",
    "    info['obs_exp'] = -1\n",
    "    chrom, pos = info['hg19_chr'], info['hg19_pos(1-based)']\n",
    "    ref, alt = info['ref'], info['alt']\n",
    "    for row in f_MPC.fetch(chrom, int(pos)-1, int(pos)+1):# 0-based inputin .fetch\n",
    "        row = row.split('\\t')\n",
    "        if row[2] == ref and row[3] == alt:\n",
    "            info['MPC'] = row[-1]\n",
    "            info['mis_badness'] = row[-3]\n",
    "            info['obs_exp'] = row[-4]\n",
    "    return info\n",
    "\n",
    "def add_phospho(info):\n",
    "    info['phospho_score'] = 0\n",
    "    info['phospho_cutoff'] = 0\n",
    "    info['phospho_diff'] = 0\n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    if gene in phosphorylation:\n",
    "        aapos =map(int, info['aapos'].split(';'))\n",
    "        \n",
    "        new_pos = list(aapos)\n",
    "        for pos in aapos: # pos is 1 based            \n",
    "            for i in range(1, 7): # left, right flanking of 7 bases\n",
    "                new_pos.append(pos + i)\n",
    "                new_pos.append(pos - i)\n",
    "        for pos in new_pos:\n",
    "            if pos in phosphorylation[gene]: #if phosphorylation[gene][pos]['AA'] == aaref:\n",
    "                info['phospho_score'] = phosphorylation[gene][pos]['Score']\n",
    "                info['phospho_cutoff'] = phosphorylation[gene][pos]['Cutoff']\n",
    "                info['phospho_diff'] = phosphorylation[gene][pos]['diff']\n",
    "                break\n",
    "    return info\n",
    "                          \n",
    "def add_SUMO(info):\n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    info['SUMO_score'] = 0\n",
    "    info['SUMO_cutoff'] = 0\n",
    "    info['SUMO_diff'] = 0\n",
    "    if gene in SUMO:\n",
    "        aapos =map(int, info['aapos'].split(';'))\n",
    "        new_pos = list(aapos)\n",
    "        for pos in aapos: # pos is 1 based            \n",
    "            for i in range(1, 7): # left, right flanking of 7 bases\n",
    "                new_pos.append(pos + i)\n",
    "                new_pos.append(pos - i)\n",
    "        for pos in new_pos:\n",
    "            if pos in SUMO[gene]: #if phosphorylation[gene][pos]['AA'] == aaref:\n",
    "                info['SUMO_score'] = SUMO[gene][pos]['Score']\n",
    "                info['SUMO_cutoff'] = SUMO[gene][pos]['Cutoff']\n",
    "                info['SUMO_diff'] = SUMO[gene][pos]['diff']\n",
    "                break\n",
    "    return info\n",
    "\n",
    "def add_ubiq(info):\n",
    "    # left, right 13\n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    info['ubiquitination'] = 0\n",
    "    if gene in ubiquitination:\n",
    "        aapos = map(int, info['aapos'].split(';'))\n",
    "        new_pos = list(aapos)\n",
    "        for pos in aapos: # pos is 1 based            \n",
    "            for i in range(1, 14): # left, right flanking of 13 bases\n",
    "                new_pos.append(pos + i)\n",
    "                new_pos.append(pos - i)\n",
    "        for pos in new_pos:\n",
    "            if pos in ubiquitination[gene]: #if phosphorylation[gene][pos]['AA'] == aaref:\n",
    "                info['ubiquitination'] = ubiquitination[gene][pos]\n",
    "                break\n",
    "    return info\n",
    "\n",
    "def add_interface(info):\n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    info['interface'] = 0\n",
    "    if gene in interface:\n",
    "        aapos = info['aapos'].split(';')\n",
    "        for pos in aapos:\n",
    "            pos = int(pos)\n",
    "            # AA_seq start from 0\n",
    "            protein_length = len(AA_seq[gene])\n",
    "            if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                if pos in interface[gene]:\n",
    "                    info['interface'] = 1\n",
    "    return info\n",
    "\n",
    "def add_ASA(info):\n",
    "    # add Accessible Surface Areas(ASA) score \n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    info['ASA'] = 0\n",
    "    if gene in ASA:\n",
    "        aapos = info['aapos'].split(';')\n",
    "        for pos in aapos:\n",
    "            pos = int(pos)\n",
    "            # AA_seq start from 0\n",
    "            protein_length = len(AA_seq[gene])\n",
    "            if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                if pos in ASA[gene]:\n",
    "                    info['ASA'] = ASA[gene][pos]\n",
    "    return info\n",
    "\n",
    "                                \n",
    "def choose_variants(info, prefix, gnomad_AF):\n",
    "    include_variants = False\n",
    "    if prefix in '.All.':\n",
    "        include_variants = True\n",
    "    elif prefix in '.HIS.': # HIS genes\n",
    "        if float(info['pli']) >= 0.5: \n",
    "            include_variants = True\n",
    "    elif prefix in '.HS.': # HS genes are pli < 0.5 and pli unknown\n",
    "        if float(info['pli']) < 0.5: # include variants with missing pli(-1)\n",
    "            include_variants = True  \n",
    "    if float(info['gnomad_exome']) > gnomad_AF:\n",
    "        include_variants = False\n",
    "    return include_variants\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_add_features(fin, w, \n",
    "                     wgsa_feat, add_feat, extra_feat, \n",
    "                     target_value, prefix, \n",
    "                     gnomad_AF=1, write_head=True):\n",
    "    \"\"\" function that selected colums from wgsa, add some other feathers \n",
    "        set . into 0\n",
    "    \"\"\"\n",
    "    with open(fin, 'rU') as f:\n",
    "        positive, negative = 0, 0\n",
    "        r = csv.reader(f,delimiter=',')\n",
    "        head = r.next()\n",
    "        feat_all = wgsa_feat + add_feat + extra_feat\n",
    "        feat = []\n",
    "\n",
    "        # set feature orders, order_feat first, then all other features\n",
    "        for f in order_feat:\n",
    "            if f in feat_all:\n",
    "                feat.append(f)\n",
    "        for f in feat_all:\n",
    "            if f not in feat:\n",
    "                feat.append(f)\n",
    "        if write_head:\n",
    "            w.writerow(feat)\n",
    "\n",
    "        for line in r:\n",
    "            info = dict(zip(head, line))\n",
    "            aaref, aaalt, aapos = info['aaref'], info['aaalt'], info['aapos']                \n",
    "            var_id = '_'.join([info['hg19_chr'], info['hg19_pos(1-based)'], info['ref'], info['alt']])\n",
    "            info['var_id'] = var_id\n",
    "            info['genename'] = info['genename'].split(';')[0] # pick the first gene if in many genes\n",
    "            \n",
    "            if var_id in exclude_var: continue\n",
    "\n",
    "            # exclude nonsense variants and syn\n",
    "            if aaref not in {'X', '.'} and aaalt not in {'X', '.'}: \n",
    "                # reformat wsga feat, missing value filled with 0, will -1 be better?\n",
    "                for c in wgsa_feat:\n",
    "                    if info[c] == '.':\n",
    "                        if c in {'ExAC_AF', '1000Gp3_AF'}:\n",
    "                            info[c] = 0\n",
    "                        else:\n",
    "                            info[c] = -1\n",
    "                            \n",
    "                    else:\n",
    "                        try:\n",
    "                            info[c] = float(info[c])\n",
    "                        except Exception, e:\n",
    "                            print str(e)\n",
    "                            print e\n",
    "                            print info[c]\n",
    "                        \n",
    "                # set some default value for extra_feat\n",
    "                for c in extra_feat:\n",
    "                    info[c] = info.get(c, 'NA')\n",
    "                \n",
    "                \n",
    "                # deal with added features. \n",
    "                # update SUMO/phospho scores\n",
    "                info = add_phospho(info)\n",
    "                info = add_SUMO(info)\n",
    "                info = add_interface(info)\n",
    "                info = add_ASA(info)\n",
    "                info = add_ubiq(info)\n",
    "                info['blosum62'] = matrix_score(aaref, aaalt, 'blosum62') \n",
    "                info['pam250'] = matrix_score(aaref, aaalt, 'pam250')\n",
    "                # gene specific feathers\n",
    "                info['complex_CORUM'] = 0\n",
    "                if info['genename'] in complex_CORUM:\n",
    "                    info['complex_CORUM'] = 1\n",
    "                info['preppi_counts'] = preppi.get(info['genename'], 0)\n",
    "                info = add_secondary(info)\n",
    "                info = add_gene_metric(info)\n",
    "                info = add_gc_content(info)\n",
    "                info = add_s_het(info)\n",
    "                info = add_BioPlex(info)\n",
    "                info = add_MPC(info)\n",
    "                info = add_REVEL(info)\n",
    "                info = add_target(info, target_value)\n",
    "                info = add_gnomad(info) # add gnomad WGS and WES at the same time, missing keys get zero AF\n",
    "               \n",
    "                # choose variants in HIS or HS\n",
    "                include_variants = choose_variants(info, prefix, gnomad_AF)\n",
    "                if include_variants:\n",
    "                    if info['target'] == 1:\n",
    "                        positive += 1\n",
    "                    elif info['target'] == 0:\n",
    "                        negative += 1\n",
    "                    else:\n",
    "                        print info['target'], 'sth wrong'\n",
    "                    w.writerow([info[c] for c in feat])\n",
    "    print '{} pos, {} neg'.format(positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variants excluded for training\n",
    "exclude_var = set()\n",
    "with open('../data/excluded_variants_gwas.txt') as f:\n",
    "    for line in f:\n",
    "        exclude_var.add(line.strip())  \n",
    "with open('../data/input_data.exclude.txt') as f:\n",
    "    for line in f:\n",
    "        exclude_var.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load protein related annotations ## these are dictionaries !!! not array\n",
    "SUMO = np.load('../data/protein/SUMO.npy').item() # dict of gene dict, dict of pos \n",
    "phosphorylation = np.load('../data/protein/phosphorylation.npy').item() # same format as SUMO\n",
    "ASA = np.load('../data/protein/ASA.npy').item() # dict of gene, of pos and its asa value\n",
    "ubiquitination = np.load('../data/protein/ubiquitination.npy').item()# dict of gene, of pos and its ubi value\n",
    "\n",
    "secondary = np.load('../data/protein/secondary.npy').item()# dict of gene, of pos and secondary value\n",
    "\n",
    "AA_seq = np.load('../data/protein/AA_seq.npy').item() # gene to sequence\n",
    "interface = np.load('../data/protein/interface.npy').item() # gene to positions\n",
    "preppi = np.load('../data/protein/preppi.npy').item() # gene to value\n",
    "BioPlex = np.load('../data/protein/BioPlex.npy').item()# gene to value\n",
    "s_het = np.load('../data/gene/s_het.npy').item() # gene to value\n",
    "prec = np.load('../data/gene/prec.npy').item() # gene to value\n",
    "pli = np.load('../data/gene/pli.npy').item() # gene to value\n",
    "lofz = np.load('../data/gene/lofz.npy').item() # gene to value\n",
    "domino = np.load('../data/gene/domino.npy').item() # gene to value\n",
    "\n",
    "complex_CORUM =  np.load('../data/protein/complex_CORUM.npy').item() # set of genes\n",
    "\n",
    "gnomad_af = np.load('../data/training/gnomad_af.npy').item() # variant to value\n",
    "gnomad_exome_af = np.load('../data/training/gnomad_exome_new.npy').item()# variant to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature order from correlation cluster\n",
    "order_feat = [u'MutationAssessor_score_rankscore', u'VEST3_rankscore', u'Polyphen2_HDIV_rankscore',\n",
    " u'Polyphen2_HVAR_rankscore', u'SIFT_converted_rankscore', u'PROVEAN_converted_rankscore',\n",
    " u'MetaSVM_rankscore',u'MetaLR_rankscore', u'FATHMM_converted_rankscore', u'M-CAP_rankscore',\n",
    " u'GenoCanyon_score_rankscore', u'LRT_converted_rankscore', u'Eigen-PC-raw_rankscore',\n",
    " u'Eigen-phred', u'Eigen-PC-phred', u'DANN_rankscore', u'CADD_phred', u'CADD_raw_rankscore',\n",
    " u'phyloP20way_mammalian_rankscore', u'GERP++_RS_rankscore', u'SiPhy_29way_logOdds_rankscore',\n",
    " u'phastCons100way_vertebrate_rankscore', u'fathmm-MKL_coding_rankscore', u'phyloP100way_vertebrate_rankscore',\n",
    " u'MutationTaster_converted_rankscore', u'phastCons20way_mammalian_rankscore', u'GM12878_fitCons_score_rankscore',\n",
    " u'HUVEC_fitCons_score_rankscore', u'integrated_fitCons_score_rankscore',u'H1-hESC_fitCons_score_rankscore', \n",
    " u'blosum62', u'pam250', u'SUMO_diff', u'SUMO_score', u'SUMO_cutoff', u'phospho_cutoff', u'phospho_score',\n",
    " u'phospho_diff', u'lofz', u'prec', u'pli', u'domino', \n",
    " u's_het', u's_het_log', u'secondary_E', u'secondary_H', u'complex_CORUM', u'preppi_counts', 'haiyuan_interface', \n",
    " u'1000Gp3_AF', u'ExAC_AF', 'gnomad', 'gnomad_exome', u'ASA', u'secondary_C', u'gc_content', u'interface', u'ubiquitination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add feathers from WGSA and other inputs, some of them need to be excluded in future training\n",
    "rank_score_cols = ['SIFT_converted_rankscore', 'Polyphen2_HDIV_rankscore', 'Polyphen2_HVAR_rankscore', \n",
    " 'LRT_converted_rankscore', 'MutationTaster_converted_rankscore', 'MutationAssessor_score_rankscore', \n",
    " 'FATHMM_converted_rankscore', 'PROVEAN_converted_rankscore', 'VEST3_rankscore', \n",
    " 'MetaSVM_rankscore', 'MetaLR_rankscore', 'M-CAP_rankscore', \n",
    " 'CADD_raw_rankscore', 'DANN_rankscore', 'fathmm-MKL_coding_rankscore', \n",
    " 'Eigen-PC-raw_rankscore', 'GenoCanyon_score_rankscore', 'integrated_fitCons_score_rankscore', \n",
    " 'GM12878_fitCons_score_rankscore', 'H1-hESC_fitCons_score_rankscore', \n",
    " 'HUVEC_fitCons_score_rankscore', 'GERP++_RS_rankscore', \n",
    " 'phyloP100way_vertebrate_rankscore', 'phyloP20way_mammalian_rankscore', \n",
    " 'phastCons100way_vertebrate_rankscore', 'phastCons20way_mammalian_rankscore', \n",
    " 'SiPhy_29way_logOdds_rankscore']\n",
    "\n",
    "wgsa_feat = ['1000Gp3_AF', 'ExAC_AF', 'CADD_phred', 'Eigen-phred', 'Eigen-PC-phred', 'RVIS']\n",
    "wgsa_feat = wgsa_feat + rank_score_cols\n",
    "\n",
    "add_feat =  ['blosum62', 'pam250', 'SUMO_score', 'SUMO_cutoff', 'SUMO_diff',\n",
    "             'phospho_score', 'phospho_cutoff','phospho_diff', 'interface',\n",
    "             'ASA', 'pli', 'lofz', 'domino', 'complex_CORUM', 'preppi_counts',\n",
    "             'secondary_H', 'secondary_C', 'secondary_E', 'ubiquitination',\n",
    "             'prec', 's_het_log', 'gc_content', 'gnomad', 'gnomad_exome', 'BioPlex',\n",
    "             'obs_exp', 'mis_badness', 'MPC', 'REVEL', \n",
    "             'target'] \n",
    "              \n",
    "# feathers used for future info\n",
    "extra_feat = ['hg19_chr', 'hg19_pos(1-based)', \n",
    "              'ref', 'alt', 'aaref','aaalt','category', 'source','INFO', 'disease', \n",
    "              'genename', 'Ensembl_transcriptid', 'var_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add anno to training files/testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefixs = ['.HIS.', '.HS.']\n",
    "for prefix in prefixs:\n",
    "    # positive training files\n",
    "    fins = ['../data/training/HGMD_DM_missense_norecceive.rare.csv',\n",
    "            '../data/training/MPC_train.rare.csv' , # only 400 HIS variants\n",
    "            '../data/training/clinvar_pathogenic_1-4star.rare.csv']\n",
    "    \n",
    "    fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "    for fin, fout in zip(fins, fouts):\n",
    "        print fout\n",
    "        with open(fout, 'w') as fw:\n",
    "            w = csv.writer(fw)\n",
    "            target_value = 1\n",
    "            sel_add_features(fin, w, \n",
    "                            wgsa_feat, add_feat, extra_feat, \n",
    "                            target_value, prefix, \n",
    "                            gnomad_AF = 0.0001, write_head=True)\n",
    "            \n",
    "    # negative training files\n",
    "    fins = ['../data/training/DiscovEHR_missense_sel.rare.csv',\n",
    "            '../data/training/DiscovEHR_rare_missense_30000.csv',\n",
    "            '../data/training/CADD_neg_train.anno.rare.csv']\n",
    "\n",
    "    fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "    for fin, fout in zip(fins, fouts):\n",
    "        print fout\n",
    "        with open(fout, 'w') as fw:\n",
    "            w = csv.writer(fw)\n",
    "            target_value = 0\n",
    "            sel_add_features(fin, w, \n",
    "                            wgsa_feat, add_feat, extra_feat, \n",
    "                            target_value, prefix, \n",
    "                            gnomad_AF = 0.0001, write_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metaSVM and other testing\n",
    "prefixs = ['.HIS.', '.HS.']\n",
    "for prefix in prefixs:\n",
    "    fins = ['../data/metaSVM/metaSVM_train.anno.rare.csv', \n",
    "            '../data/metaSVM/metaSVM_test1.anno.rare.csv', \n",
    "            '../data/metaSVM/metaSVM_test2.anno.rare.csv', \n",
    "            '../data/metaSVM/metaSVM_test3.anno.rare.csv', \n",
    "            '../data/metaSVM/metaSVM_addtest1.anno.rare.csv', \n",
    "            '../data/metaSVM/metaSVM_addtest2.anno.rare.csv',\n",
    "            '../data/cancer_hotspots/cancer_sel.csv',\n",
    "            '../data/cancer_hotspots/hotspot_paper_randomsel.anno.rare.final.csv']\n",
    "    \n",
    "    fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "    for fin, fout in zip(fins, fouts):\n",
    "        with open(fout, 'w') as fw:\n",
    "            print fout\n",
    "            w = csv.writer(fw)\n",
    "            target_value = 'NA'\n",
    "            sel_add_features(fin, w, \n",
    "                            wgsa_feat, add_feat, extra_feat, \n",
    "                            target_value, prefix,\n",
    "                            gnomad_AF = 0.0001,\n",
    "                            write_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add annotation to de novo case and control sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark0315.anno.rare.final.csv ../data/case_control/spark0315.anno.rare.final.HIS.reformat.csv 1\n",
      "149 pos, 0 neg\n",
      "spark0315.anno.rare.final.csv ../data/case_control/spark0315.anno.rare.final.HS.reformat.csv 1\n",
      "244 pos, 0 neg\n"
     ]
    }
   ],
   "source": [
    "# de novo variants\n",
    "prefixs = ['.HIS.', '.HS.']#, '.All.']\n",
    "for prefix in prefixs:\n",
    "    # control\n",
    "    fcontrol = ['../data/case_control/control_1911.anno.rare.final.csv',\n",
    "                '../data/case_control/control_MarkDaly.anno.rare.final.csv']\n",
    "    \n",
    "    # case\n",
    "    fcase = ['../data/case_control/asd.anno.rare.final.csv', \n",
    "             '../data/case_control/DDD_new_0.2.anno.rare.final.csv',\n",
    "             '../data/case_control/chd_yale.anno.rare.final.csv',\n",
    "             '../data/case_control/CHD_DDD_YALE.anno.rare.final.csv',\n",
    "             '../data/case_control/case_MarkDaly.anno.rare.final.csv',\n",
    "             '../data/case_control/spark.anno.rare.final.csv',\n",
    "             '../data/case_control/SSC_Xueya.anno.rare.final.csv',\n",
    "             '../data/case_control/spark0315.anno.rare.csv']\n",
    "\n",
    "    \n",
    "    for fins, target in zip([fcase, fcontrol], [1, 0]):\n",
    "        fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "        for fin, fout in zip(fins, fouts):\n",
    "            with open(fout, 'w') as fw:\n",
    "                print fin.split('/')[-1], fout, target\n",
    "                w = csv.writer(fw)\n",
    "                target_value = target\n",
    "                sel_add_features(fin, w, \n",
    "                                wgsa_feat, add_feat, extra_feat, \n",
    "                                target_value, prefix,\n",
    "                                gnomad_AF=1.0, # filter in future analysis\n",
    "                                write_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefixs = ['.HIS.', '.HS.']\n",
    "for prefix in prefixs:\n",
    "\n",
    "    # negative\n",
    "    fins = ['../data/training/DiscovEHR_missense.rare.csv']\n",
    "\n",
    "    fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "    for fin, fout in zip(fins, fouts):\n",
    "        print fout\n",
    "        with open(fout, 'w') as fw:\n",
    "            w = csv.writer(fw)\n",
    "            target_value = 0\n",
    "            sel_add_features(fin, w, \n",
    "                            wgsa_feat, add_feat, extra_feat, \n",
    "                            target_value, prefix, \n",
    "                            gnomad_AF = 0.0001, \n",
    "                            write_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process all missense and all cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all cancer hostspot and other files, keep all 1% variants \n",
    "\n",
    "prefix = '.All.'\n",
    "fins = ['/data/hq2130/large_files/rare_missense_id.anno.rare.csv']\n",
    "fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "for fin, fout in zip(fins, fouts):\n",
    "    with open(fout, 'w') as fw:\n",
    "        w = csv.writer(fw)\n",
    "        target_value = 0\n",
    "        sel_add_features(fin, w, wgsa_feat,\n",
    "                         add_feat, extra_feat, \n",
    "                         target_value, prefix, \n",
    "                         gnomad_AF = 0.01, write_head=True)\n",
    "\n",
    "print \"done\" "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_section_display": "block",
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
